{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Object Detection via Sensor Fusion (Lidar and Camera) \n",
    "\n",
    "Originaly written by: **PixelOverflow**\n",
    "\n",
    "If you wish to watch videos from you can find videos in the following links:\n",
    "### Sensor Fusion Tutorial:\n",
    "\n",
    "- Part 1 - [3D Object Detection Overview](https://www.youtube.com/watch?v=hXpXKRnnM9o&t=0s)\n",
    "- Part 2 - [Coordinate Transformations](https://www.youtube.com/watch?v=EfiYr61RGUA&t=0s) \n",
    "- Part 3 - [Loading Calibration Data](https://www.youtube.com/watch?v=pRAPXfWy-3A&t=0s)     \n",
    "- Part 4 - [Sensor Fusion Pipeline](https://www.youtube.com/watch?v=vVtpKzEwEFM&t=0s)  \n",
    "- Part 5 - [Check the Math](https://www.youtube.com/watch?v=lpjQnIrnt20&t=0s)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will dive into the KITTI dataset and detect objects in 3D using Early Sensor Fusion or Early Fusion which aims to fuse raw data from multiple sources and then perform detection. Late fusion on the other hand involves first detecting objects, and then fusing the detections. In this case we will perform a modified fusion, where we detect objects in the camera images and then fuse their centers with the LiDAR data to get depth.\n",
    "\n",
    "The main steps are summarized as:\n",
    "\n",
    "- Detect objects in the camera images (Detection)\n",
    "- Project 3D LiDAR point clouds to 2D Image space (Fusion)\n",
    "- Associate LiDAR depth with each Detected object (Association to get Depth)\n",
    "- Detection in 3D as opposed to 2D is much more useful to an autonomous vehicle since 3D detection allows the system know where objects are physically located in the world.\n",
    "\n",
    "\n",
    "For more information a readme for the KITTI data can be found [here](https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT), and a paper that details the data collection and coordinate systems can be found [here](http://www.cvlibs.net/publications/Geiger2013IJRR.pdf).\n",
    "\n",
    "\n",
    "Now let's get the data and get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_sync.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip them\n",
    "!unzip  2011_10_03_drive_0047_sync.zip\n",
    "!unzip  2011_10_03_calib.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Library Import\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import KITTI Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_utils.py\n",
    "from kitti_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "In the KITTI raw dataset we get images from four cameras (two grayscale and two RGB), the velodyne LiDAR, and the OXTS GPS navigation system.\n",
    "\n",
    "The update rates are as follows:\n",
    "\n",
    "- RGB camera: 15 Hz (15 fps)\n",
    "- OXTS GPS navigation system: 100Hz\n",
    "- Velodyne LiDAR: 10Hz\n",
    "\n",
    "The data is synched to the LiDAR, since it has the lowest update rate, but the sync between the camera, GPS/IMU (navigation), and LiDAR is not precise (even though we are using the synched raw data!). Per the KITTI [description](http://www.cvlibs.net/publications/Geiger2013IJRR.pdf) the worst time difference between the camera/velodyne and gps/imu is at most 5ms. More precise measurements can be obtained with interpolation, but for simplicity we will neglect these differences since the small error from the imprecise sync will not greatly impact our measurements. We will see later when we project LiDAR points onto the camera images, that there is no noticable difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the paths to all of the datafiles, the RGB images are standard .png's, the Navigation frames are .txt files, but the LiDAR point clouds are binary files. The [KITTI README](https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT) describes the structure of the binary files, and we will import a utility function to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of left images: 837\n",
      "Number of right images: 837\n",
      "Number of LiDAR point clouds: 837\n",
      "Number of GPS/IMU frames: 837\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA = r'2011_10_03/2011_10_03_drive_0047_sync'\n",
    "\n",
    "# Get RGB camera data\n",
    "left_image_paths = sorted(glob(os.path.join(PATH_DATA, 'image_02/data/*.png')))\n",
    "right_image_paths = sorted(glob(os.path.join(PATH_DATA, 'image_03/data/*.png')))\n",
    "\n",
    "# Get LiDAR data\n",
    "bin_paths = sorted(glob(os.path.join(PATH_DATA, 'velodyne_points/data/*.bin')))\n",
    "\n",
    "# Get GPS/IMU data\n",
    "oxts_path = sorted(glob(os.path.join(PATH_DATA, r'oxts/data**/*.txt')))\n",
    "\n",
    "print(f\"Number of left images: {len(left_image_paths)}\")\n",
    "print(f\"Number of right images: {len(left_image_paths)}\")\n",
    "print(f\"Number of LiDAR point clouds: {len(left_image_paths)}\")\n",
    "print(f\"Number of GPS/IMU frames: {len(left_image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera/LiDAR/IMU Data\n",
    "\n",
    "In order to obtain an understanding of the code to follow, it will help to cover the different reference frames that we will be working with and how we can convert between them. The Camera, LiDAR, and IMU are located at different positions on the vehicle and all have different reference frames. In autonomous research, the main vehicle that is collecting perception data (camera/LiDAR) is usually called the ego vehicle.\n",
    "\n",
    "- camera\n",
    "\n",
    "    - x → right\n",
    "    - y → down\n",
    "    - z → forward\n",
    "\n",
    "- LiDAR\n",
    "\n",
    "    - x → forward\n",
    "    - y → left\n",
    "    - z → up\n",
    "\n",
    "- IMU\n",
    "\n",
    "    - x → forward\n",
    "    - y → left\n",
    "    - z → up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniaz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
